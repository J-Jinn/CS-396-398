{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Logits and Text for use in Data Visualization\n",
    "## Author: Joseph Jinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- https://github.com/dunovank/jupyter-themes\n",
    " - (Jupyter Notebook Themes)\n",
    "\n",
    "- https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29\n",
    " - (useful additions for Jupyter Notebook)\n",
    "\n",
    "- https://medium.com/@rbmsingh/making-jupyter-dark-mode-great-5adaedd814db\n",
    " - (Jupyter dark-mode settings - my eyes are no longer bleeding...)\n",
    "\n",
    "- https://github.com/ipython-contrib/jupyter_contrib_nbextensions\n",
    " - (Jupyter extensions)\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    " - (PyTorch tutorial on character-level RNN)\n",
    " \n",
    "<br>\n",
    "\n",
    "Enter this in Terminal (for use with jupyter-themes):\n",
    "\n",
    "jt -t monokai -f fira -fs 13 -nf ptsans -nfs 11 -N -kl -cursw 5 -cursc r -cellw 95% -T\n",
    "\n",
    "<br>\n",
    "\n",
    "Important files to reference:\n",
    "\n",
    "- modeling_gpt2.py\n",
    " - The GPT2 model source code.\n",
    " \n",
    "- tokenization_gpy2.py\n",
    " - The tokenizer class for the GPT2 model.\n",
    " \n",
    " <br>\n",
    " \n",
    "Reference Material to understand the Theoretical Foundation of GPT2:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Language_model\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "It would also be helpful to have some concept about beam search… I’m not super-happy with what my Googling obtains but…\n",
    "\n",
    "https://en.wikipedia.org/wiki/Beam_search\n",
    "\n",
    "https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    " \n",
    " <br>\n",
    " \n",
    "Also maybe helpful but don’t get distracted:\n",
    "\n",
    "the first 20 minutes or so of this (everything after that is details of training, skip it.)  \n",
    "\n",
    "https://www.youtube.com/watch?v=Keqep_PKrY8\n",
    "\n",
    "https://medium.com/syncedreview/language-model-a-survey-of-the-state-of-the-art-technology-64d1a2e5a466\n",
    "\n",
    "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "\n",
    "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import trange # Instantly make your loops show a smart progress meter\n",
    "\n",
    "import torch # Pytorch.\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # Numpy.\n",
    "import pandas as pd # Pandas.\n",
    "\n",
    "###############################################\n",
    "\n",
    "# Hugging-face Transformers.\n",
    "from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from transformers import CTRLLMHeadModel, CTRLTokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the GPT2-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = GPT2LMHeadModel # Specifies the model to use.\n",
    "tokenizer_class = GPT2Tokenizer # Specifies the tokenizer to use for the model.\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2') # Use pre-trained model.\n",
    "model = model_class.from_pretrained('gpt2') # User pre-trained model.\n",
    "model.to('cpu') # Specifies what machine to run the model on.\n",
    "model.eval() # Specifies that the model is NOT in training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT2 tokenizer converts a string into a list of ID's and back when decoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: [1820, 4004, 1332, 7442, 487, 73, 69, 7568, 70, 73, 456, 70, 42421]\n",
      "\n",
      "Decoded string: ['my', ' favorite', ' test', 'case', 'ff', 'j', 'f', 'df', 'g', 'j', 'gh', 'g', 'kj']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_string = tokenizer.encode('my favorite testcaseffjfdfgjghgkj')\n",
    "print(f\"Encoded string: {encoded_string}\\n\")\n",
    "\n",
    "# Decodes token-by-token, instead of decoding the entire string all at once.\n",
    "decoded_string = [tokenizer.decode(tok) for tok in encoded_string]\n",
    "print(f\"Decoded string: {decoded_string}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Barbones GPT2-Model Text Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape: torch.Size([3])\n",
      "Context converted to Pytorch Tensor object: tensor([ 4798, 10786, 15496])\n",
      "\n",
      "Context shape after 'unsqueeze': torch.Size([1, 3])\n",
      "Context after 'unsqueeze': tensor([[ 4798, 10786, 15496]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    debug = True # Enable or disable debug print statements.\n",
    "    temperature = 1 # Default value.\n",
    "    length = 1 # Default value.\n",
    "    num_samples = 1 # Default value.\n",
    "    \n",
    "    # Raw text string.\n",
    "    raw_text = \"print('Hello\"\n",
    "    \n",
    "    # Encode raw text.\n",
    "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "    \n",
    "    context = context_tokens # Re-name.\n",
    "    \n",
    "    # Convert to a PyTorch Tensor object (numpy array).\n",
    "    context = torch.tensor(context, dtype=torch.long, device='cpu')\n",
    "    if debug:\n",
    "        print(f\"Context shape: {context.shape}\")\n",
    "        print(f\"Context converted to Pytorch Tensor object: {context}\\n\")\n",
    "        \n",
    "    # Unsqueeze adds a dimension to the Tensor array.\n",
    "    # Repeat adds x-dimensions and repeats the Tensor elements y-times.\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    if debug:\n",
    "        print(f\"Context shape after 'unsqueeze': {context.shape}\")\n",
    "        print(f\"Context after 'unsqueeze': {context}\\n\")\n",
    "        \n",
    "    generated = context # Re-name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test outputs Tensor shape: torch.Size([1, 1, 5])\n",
      "Test outputs Tensor object:: (tensor([[[1, 2, 3, 4, 5]]]),)\n",
      "\n",
      "Test next token logits shape: torch.Size([1, 5])\n",
      "Test next token logits: tensor([[1, 2, 3, 4, 5]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a toy outputs data structure.\n",
    "test_outputs = tuple(torch.tensor([[[[1, 2, 3, 4, 5]]]]))\n",
    "if debug:\n",
    "    print(f\"Test outputs Tensor shape: {list(test_outputs)[0].shape}\")\n",
    "    print(f\"Test outputs Tensor object:: {test_outputs}\\n\")\n",
    "\n",
    "# Create a toy next token logits data structure.\n",
    "test_next_token_logits = test_outputs[0][:, -1, :]\n",
    "if debug:\n",
    "    print(f\"Test next token logits shape: {test_next_token_logits.shape}\")\n",
    "    print(f\"Test next token logits: {test_next_token_logits}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adapted for use in Data Visualization.\n",
    "\n",
    "Note: Command Mode - L (adds line numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape: torch.Size([1, 3, 50257])\n",
      "\n",
      "Outputs: tensor([[[-31.0651, -30.1248, -33.1206,  ..., -38.5258, -38.8258, -31.3107],\n",
      "         [-64.8712, -63.1647, -61.1237,  ..., -75.9139, -74.8393, -66.0857],\n",
      "         [-50.9404, -57.3028, -59.3610,  ..., -65.5572, -64.8027, -58.9236]]])\n",
      "\n",
      "Outputs 3rd-level nested list within tensor object 1st-element shape: torch.Size([50257])\n",
      "\n",
      "Outputs 3rd-level nested list within tensor object 1st-element: tensor([-31.0651, -30.1248, -33.1206,  ..., -38.5258, -38.8258, -31.3107])\n",
      "\n",
      "Outputs 3rd-level nested list within tensor object 2nd-element shape: torch.Size([50257])\n",
      "\n",
      "Outputs 3rd-level nested list within tensor object 2nd-element: tensor([-64.8712, -63.1647, -61.1237,  ..., -75.9139, -74.8393, -66.0857])\n",
      "\n",
      "Next token logits shape: torch.Size([1, 50257])\n",
      "\n",
      "Next token logits head: -50.940406799316406\n",
      "Next token logits: tensor([[-50.9404, -57.3028, -59.3610,  ..., -65.5572, -64.8027, -58.9236]])\n",
      "\n",
      "df shape: (1, 50257)\n",
      "df_transposed shape: (50257, 1)\n",
      "wordID length: 50257\n",
      "df_transposed head:\n",
      "    wordScore  wordID\n",
      "0 -50.940407       0\n",
      "1 -57.302753       1\n",
      "2 -59.360977       2\n",
      "3 -56.790993       3\n",
      "4 -54.606598       4\n",
      "5 -59.316162       5\n",
      "6 -51.493431       6\n",
      "7 -55.806496       7\n",
      "8 -53.727768       8\n",
      "9 -57.877201       9\n",
      "Word ID: 10\n",
      "!\n",
      "df_test shape: (10, 2)\n",
      "df_transposed head:\n",
      "    wordScore  wordID decodedWord\n",
      "0 -50.940407       0           !\n",
      "1 -57.302753       1           \"\n",
      "2 -59.360977       2           #\n",
      "3 -56.790993       3           $\n",
      "4 -54.606598       4           %\n",
      "5 -59.316162       5           &\n",
      "6 -51.493431       6           '\n",
      "7 -55.806496       7           (\n",
      "8 -53.727768       8           )\n",
      "9 -57.877201       9           *\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # This specifies not to use stochastic gradient descent!\n",
    "#     for _ in trange(length): \n",
    "\n",
    "    \"\"\"\n",
    "    Based on the shape, outputs is a 3-dimensional structure.\n",
    "    1st dimension: a single column storing the 12 sets of the same vocabulary.\n",
    "    2nd dimension: each set of the vocabulary as a row.\n",
    "    3rd dimension: the individual words in the vocabulary with their assigned scores.\n",
    "    \n",
    "    Important (if I understand correctly):\n",
    "    \n",
    "    The indices represent the encoded ID values of the word.\n",
    "    The element value represent the score assigned to each word (how likely to be the next word).\n",
    "    \"\"\"\n",
    "    # Call to GPT2 model generates a Tensor object containing \"scores\" for the entire vocabulary.\n",
    "    outputs = model(input_ids=generated)\n",
    "    if debug:\n",
    "        print(f\"Outputs shape: {list(outputs)[0].shape}\\n\")\n",
    "        print(f\"Outputs: {list(outputs)[0]}\\n\") # Outputs is a tensor containing a lot of stuff...\n",
    "\n",
    "        print(f\"Outputs 3rd-level nested list within tensor object 1st-element shape: {list(outputs)[0][0][0].shape}\\n\")\n",
    "        print(f\"Outputs 3rd-level nested list within tensor object 1st-element: {list(outputs)[0][0][0]}\\n\")\n",
    "        print(f\"Outputs 3rd-level nested list within tensor object 2nd-element shape: {list(outputs)[0][0][1].shape}\\n\")\n",
    "        print(f\"Outputs 3rd-level nested list within tensor object 2nd-element: {list(outputs)[0][0][1]}\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    [:, -1, :]\n",
    "    : indexes everything in that dimension.\n",
    "    -1 indexes the last element in that dimension.\n",
    "    \n",
    "    outputs[0] indexes into the 12 sets of vocabulary.\n",
    "    \n",
    "    next_token_logits is the last of the 12 sets of vocabulary.\n",
    "    \"\"\"\n",
    "    next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "    if debug:\n",
    "        print(f\"Next token logits shape: {next_token_logits.shape}\\n\")\n",
    "        print(f\"Next token logits head: {next_token_logits[0][0]}\")\n",
    "        print(f\"Next token logits: {next_token_logits}\\n\")\n",
    "\n",
    "    filtered_logits = next_token_logits # Set to default name from run_generation.py\n",
    "    \n",
    "    \"\"\"\n",
    "    Section for converting PyTorch Tensor to Pandas Dataframe via Numpy.\n",
    "    \"\"\"\n",
    "    numpy_array = next_token_logits.numpy()\n",
    "    df = pd.DataFrame(numpy_array)\n",
    "    print(f\"df shape: {df.shape}\")\n",
    "    df_tranposed = df.transpose()\n",
    "    print(f\"df_transposed shape: {df_tranposed.shape}\")\n",
    "    df_tranposed.columns = [\"wordScore\"]\n",
    "        \n",
    "    # Determine word ID's (also index values)\n",
    "    wordID = []\n",
    "    for i in range(0, len(next_token_logits[0])):\n",
    "        wordID.append(i)\n",
    "    print(f\"wordID length: {len(wordID)}\")\n",
    "        \n",
    "    # Assign word ID's to each word score.\n",
    "    df_tranposed['wordID'] = wordID\n",
    "    print(f\"df_transposed head:\\n {df_tranposed.head(10)}\")\n",
    "    \n",
    "    # Test that we can decode a single wordID from the Pandas dataframe.\n",
    "    print(f\"Word ID: {df_tranposed['wordID'][10]}\")\n",
    "    print(tokenizer.decode(int(df_tranposed['wordID'][0])))\n",
    "    \n",
    "    # Debugging purposes.\n",
    "    df_test = pd.DataFrame(df_tranposed.head(10))\n",
    "    print(f\"df_test shape: {df_test.shape}\")\n",
    "  \n",
    "    def decode_word(row):\n",
    "        \"\"\"\n",
    "        Function decodes the word ID and add decoded word as entry in a new column.\n",
    "        \n",
    "        params:\n",
    "            row - a panda series.\n",
    "        return:\n",
    "            row[\"decodedWord\"] - decoded word as new entry in column.\n",
    "        \"\"\"\n",
    "#         print(f\"Type: {type(row['wordID'])}\")\n",
    "#         print(f\"Type conversion: {type(int(row['wordID']))}\")\n",
    "        myID = row['wordID'].astype(int)\n",
    "#         print(f\"ID: {myID}\")\n",
    "        decoded_word = tokenizer.decode(int(myID))\n",
    "#         print(f\"Decoded word: {decoded_word}\")\n",
    "        row[\"decodedWord\"] = decoded_word\n",
    "        return row[\"decodedWord\"]\n",
    "    \n",
    "    # Apply function to every row in dataframe.\n",
    "    df_tranposed[\"decodedWord\"] = df_tranposed.apply(decode_word, axis=1)\n",
    "    print(f\"df_transposed head:\\n {df_tranposed.head(10)}\")\n",
    "\n",
    "    df_tranposed.to_csv(f'D:/Dropbox/GitHub-Pages/J-Jinn.github.io/huggingface_transformers/next_token_logits.csv', \n",
    "                        index = None, header=True, encoding=\"utf-8\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My torch.topk object: torch.return_types.topk(\n",
      "values=tensor([[-47.6363, -47.9428, -48.5424, -50.3235, -50.8331, -50.8338, -50.8381,\n",
      "         -50.9404, -51.4934, -51.5334, -51.6169, -51.8034, -51.8455, -52.1599,\n",
      "         -52.2399, -52.4960, -52.8470, -52.8655, -52.9795, -53.0174, -53.0947,\n",
      "         -53.1981, -53.2480, -53.4509, -53.5328, -53.5375, -53.5451, -53.5861,\n",
      "         -53.6293, -53.6468]]),\n",
      "indices=tensor([[ 2159,    11,   995,   422, 11537,  3256, 10603,     0,     6, 13679,\n",
      "           612,   705,    13,  4064, 24036,  4032,   314,    12,  1770,   198,\n",
      "           616,    25,  2506,   290, 29564,  6894, 21168,   532, 28265,   262]]))\n",
      "\n",
      "torch.topk indices: tensor([[ 2159,    11,   995,   422, 11537,  3256, 10603,     0,     6, 13679,\n",
      "           612,   705,    13,  4064, 24036,  4032,   314,    12,  1770,   198,\n",
      "           616,    25,  2506,   290, 29564,  6894, 21168,   532, 28265,   262]])\n",
      "\n",
      "torch.topk values: tensor([[-47.6363, -47.9428, -48.5424, -50.3235, -50.8331, -50.8338, -50.8381,\n",
      "         -50.9404, -51.4934, -51.5334, -51.6169, -51.8034, -51.8455, -52.1599,\n",
      "         -52.2399, -52.4960, -52.8470, -52.8655, -52.9795, -53.0174, -53.0947,\n",
      "         -53.1981, -53.2480, -53.4509, -53.5328, -53.5375, -53.5451, -53.5861,\n",
      "         -53.6293, -53.6468]])\n",
      "\n",
      "\n",
      "Decoded torch.topk values: [' World', ',', ' world', ' from', \"')\", \"',\", 'World', '!', \"'\", \"!'\", ' there', \" '\", '.', ' %', \"');\", \",'\", ' I', '-', ' Mr', '\\n', ' my', ':', ' everyone', ' and', ' WORLD', 'world', ' Kitty', ' -', '!,', ' the']\n",
      "\n",
      "\n",
      "Decoded torch.topk values:  World, world from')',World!'!' there '. %');,' I- Mr\n",
      " my: everyone and WORLDworld Kitty -!, the\n",
      "\n",
      " World\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.topk performs a similar function to Softmax.\n",
    "    Use the words' \"scores\" to choose the top \"k\" most likely predicted words (tokens).\n",
    "\n",
    "- torch.topk\n",
    " - Returns the :attr:`k` largest elements of the given :attr:`input` tensor along a given dimension.\n",
    " \n",
    "Non-statistical and probabilistic method, so results are deterministic (always the same).\n",
    "\"\"\"\n",
    "# Return the top \"k\" most likely (highest score value) words in sorted order..\n",
    "my_topk = torch.topk(input=filtered_logits, k=30, dim=1, sorted=True)\n",
    "print(f\"My torch.topk object: {my_topk}\\n\")\n",
    "print(f\"torch.topk indices: {my_topk.indices}\\n\")\n",
    "print(f\"torch.topk values: {my_topk.values}\\n\")\n",
    "\n",
    "# https://stackoverflow.com/questions/34750268/extracting-the-top-k-value-indices-from-a-1-d-tensor\n",
    "# https://stackoverflow.com/questions/53903373/convert-pytorch-tensor-to-python-list\n",
    "\n",
    "# Indices = encoded words, Values = scores.\n",
    "print(f\"\\nDecoded torch.topk values: {[tokenizer.decode(idx) for idx in my_topk.indices.squeeze().tolist()]}\\n\")\n",
    "print(f\"\\nDecoded torch.topk values: {tokenizer.decode(my_topk.indices.squeeze().tolist())}\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Note: Index values correspond to word ID's\n",
    "\"\"\"\n",
    "# for i in range(0, len(next_token_logits[0])):\n",
    "#     print(tokenizer.decode(i))\n",
    "#     if i > 100:\n",
    "#         break\n",
    "print(tokenizer.decode(2159))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method of sampling simply returns the word with the highest \"score\" as the chosen next token.\n",
    "(non-statistical and probabilistic method)\n",
    "\n",
    "The result is deterministic (always the same).\n",
    "\"\"\"\n",
    "next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1) # Greedy sampling.\n",
    "\n",
    "if debug:\n",
    "    print(f\"Next token shape: {next_token.shape}\\n\")\n",
    "    print(f\"Next token: {next_token}\\n\")\n",
    "    print(f\"Decoded next token(s): {tokenizer.decode(next_token.squeeze().tolist())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Monotonic (always) increasing function used by Softmax: e^x\n",
    "\n",
    "Below is the latex equation from the API documentation for Softmax:\n",
    "    math:`\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}`\n",
    "\n",
    "Translation: \n",
    "    Each word's score is converted to a probability using: e^(x_i) / sum of e^(x_j) for all words.\n",
    "    \n",
    "Token is then chosen based on the multinomial probability distribution of all the words' probabilities.\n",
    "\n",
    "The result is non-deterministic since statistical probabilistic method based on random sampling.\n",
    "\"\"\"\n",
    "# Note: num_samples determines the number of tokens to choose.\n",
    "next_token = torch.multinomial(\n",
    "    F.softmax(filtered_logits, dim=-1),\n",
    "    num_samples=1) # Not greedy?\n",
    "\n",
    "if debug:\n",
    "    print(f\"Next token shape: {next_token.shape}\\n\")\n",
    "    print(f\"Next token: {next_token}\\n\")\n",
    "    print(f\"Decoded next token(s): {tokenizer.decode(next_token.squeeze().tolist())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the chosen token (predicted word) to the end of the tokenized (encoded) string.\n",
    "generated = torch.cat((generated, next_token), dim=1)\n",
    "if debug:\n",
    "    print(f\"Generated shape: {generated.shape}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(f\"Decoded 'generated' tokens: {tokenizer.decode(generated.squeeze().tolist())}\\n\")\n",
    "            \n",
    "out = generated # Re-name.\n",
    "if debug:\n",
    "    print(f\"Contents of 'out': {out}\")\n",
    "\n",
    "# This line removes the original text but keeps appending the generated words one-by-one (based on iteration length).\n",
    "out = out[:, len(context_tokens):].tolist()\n",
    "if debug:\n",
    "    print(f\"Contents of 'out' after .tolist(): {out}\\n\")\n",
    "    print(f\"Length of context tokens:{len(context_tokens)}\\n\")\n",
    "\n",
    "# Outputs the result of the text modeling and prediction.\n",
    "for o in out:\n",
    "    # Decode - convert from token ID's back into English words.\n",
    "    text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "#     text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "    print(f\"Content of text: ##{text}##\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X = X_1, X_2, ..., X_100\n",
    "IID random sampling? - with replacement.\n",
    "SRS - simple random sampling? - without replacement.\n",
    "\n",
    "Binomial coefficients - n choose k (without replacement, order doesn't matter)\n",
    "Permutations - n^k (with replacement, order matters)\n",
    "Permutations - (k + n - 1) choose n (with replacement, order matters)\n",
    "\n",
    "Note to self: Know multinomial probability distributions since basis for everything.\n",
    "\"\"\"\n",
    "# \".\" forces conversion to floats from integers.\n",
    "# Softmax function accepts input Tensor, number of random samples to take, and whether to sample with replacement.\n",
    "torch.multinomial(F.softmax(torch.Tensor([3., -1, 2])), 100, replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same as above code cells, but refactored into a function in order to make multiple predictions for user-inputted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(context):\n",
    "    \"\"\"\n",
    "    This function generates the predicted text and returns the text as tokens.\n",
    "\n",
    "    Parameters: None\n",
    "    Return: generated - PyTorch Tensor containing tokens.\n",
    "    \"\"\"\n",
    "    make_prediction_debug = False\n",
    "    temperature = 1 # Default value.\n",
    "    length = 20 # Default value.\n",
    "    num_samples = 1 # Default value.\n",
    "    \n",
    "    # Convert to a PyTorch Tensor object (numpy array).\n",
    "    context = torch.tensor(context, dtype=torch.long, device='cpu')\n",
    "    if make_prediction_debug:\n",
    "        print(f\"Context shape: {context.shape}\")\n",
    "        print(f\"Context converted to Pytorch Tensor object: {context}\\n\")\n",
    "        \n",
    "    # Unsqueeze adds a dimension to the Tensor array.\n",
    "    # Repeat adds x-dimensions and repeats the Tensor elements y-times.\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    if make_prediction_debug:\n",
    "        print(f\"Context shape after 'unsqueeze': {context.shape}\")\n",
    "        print(f\"Context after 'unsqueeze': {context}\\n\")\n",
    "    \n",
    "    generated = context # Set to name as in run_generation.py\n",
    "    \n",
    "    ############################################################################################\n",
    "\n",
    "    with torch.no_grad(): # This specifies not to use stochastic gradient descent!\n",
    "        for _ in trange(length): \n",
    "            \n",
    "            # Call to GPT2 model generates a Tensor object containing \"scores\" for the entire vocabulary.\n",
    "            outputs = model(input_ids=generated)\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Outputs shape: {list(outputs)[0].shape}\\n\")\n",
    "                print(f\"Outputs: {list(outputs)[0]}\\n\") # Outputs is a tensor containing a lot of stuff...\n",
    "\n",
    "                print(f\"Outputs 3rd-level nested list within tensor object 1st-element: {list(outputs)[0][0][0].shape}\\n\")\n",
    "                print(f\"Outputs 3rd-level nested list within tensor object 1st-element: {list(outputs)[0][0][0]}\\n\")\n",
    "                print(f\"Outputs 3rd-level nested list within tensor object 2nd-element: {list(outputs)[0][0][1].shape}\\n\")\n",
    "                print(f\"Outputs 3rd-level nested list within tensor object 2nd-element: {list(outputs)[0][0][1]}\\n\")\n",
    "\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Next token logits shape: {next_token_logits.shape}\\n\")\n",
    "                print(f\"Next token logits: {next_token_logits}\\n\")\n",
    "\n",
    "            filtered_logits = next_token_logits # Set to default name from run_generation.py\n",
    "            \n",
    "            ############################################################################################\n",
    "            \n",
    "            \"\"\"\n",
    "            This method of sampling simply returns the word with the highest \"score\" as the chosen next token.\n",
    "            (non-statistical and probabilistic method)\n",
    "\n",
    "            The result is deterministic (always the same).\n",
    "            \"\"\"\n",
    "            next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1) # Greedy sampling.\n",
    "\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Next token shape: {next_token.shape}\\n\")\n",
    "                print(f\"Next token: {next_token}\\n\")\n",
    "                print(f\"Decoded next token(s): {tokenizer.decode(next_token.squeeze().tolist())}\\n\")\n",
    "    \n",
    "            ############################################################################################\n",
    "        \n",
    "            \"\"\"\n",
    "            Monotonic (always) increasing function used by Softmax: e^x\n",
    "\n",
    "            Below is the latex equation from the API documentation for Softmax:\n",
    "                math:`\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}`\n",
    "\n",
    "            Translation: \n",
    "                Each word's score is converted to a probability using: e^(x_i) / sum of e^(x_j) for all words.\n",
    "\n",
    "            Token is then chosen based on the multinomial probability distribution of all the words' probabilities.\n",
    "\n",
    "            The result is non-deterministic since statistical probabilistic method based on random sampling.\n",
    "            \"\"\"\n",
    "            # Note: num_samples determines the number of tokens to choose.\n",
    "            next_token = torch.multinomial(\n",
    "                F.softmax(filtered_logits, dim=-1),\n",
    "                num_samples=1) # Not greedy?\n",
    "\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Next token shape: {next_token.shape}\\n\")\n",
    "                print(f\"Next token: {next_token}\\n\")\n",
    "                print(f\"Decoded next token(s): {tokenizer.decode(next_token.squeeze().tolist())}\\n\")\n",
    "            \n",
    "            ############################################################################################\n",
    "            \n",
    "            \n",
    "            # Concatenate the chosen token (predicted word) to the end of the tokenized (encoded) string.\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Generated shape: {generated.shape}\")\n",
    "                print(f\"Generated: {generated}\")\n",
    "                print(f\"Decoded 'generated' tokens: {tokenizer.decode(generated.squeeze().tolist())}\\n\")\n",
    "                                       \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calls the make_prediction(context) function and outputs text prediction and generation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def output_prediction(num_predictions, context_tokens):\n",
    "    \"\"\"\n",
    "    This function outputs the results of our generated predicted text.\n",
    "    \"\"\"\n",
    "    output_prediction_debug = False\n",
    "    # # Prompt user for input text.\n",
    "    # raw_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
    "    # context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "\n",
    "    # out = sample_sequence(\n",
    "    #     model=model,\n",
    "    #     context=context_tokens,\n",
    "    #     num_samples=args.num_samples,\n",
    "    #     length=args.length,\n",
    "    #     temperature=args.temperature,\n",
    "    #     top_k=args.top_k,\n",
    "    #     top_p=args.top_p,\n",
    "    #     repetition_penalty=args.repetition_penalty,\n",
    "    #     is_xlnet=bool(args.model_type == \"xlnet\"),\n",
    "    #     is_xlm_mlm=is_xlm_mlm,\n",
    "    #     xlm_mask_token=xlm_mask_token,\n",
    "    #     xlm_lang=xlm_lang,\n",
    "    #     device=args.device,\n",
    "    # )\n",
    "    \n",
    "    ############################################################################################\n",
    "    \n",
    "    for i in range(0, num_predictions):\n",
    "            \n",
    "        out = make_prediction(context_tokens) # Function returns \"generated\" - PyTorch Tensor containing encoded tokens.\n",
    "        if output_prediction_debug:\n",
    "            print(f\"Contents of 'out': {out}\")\n",
    "\n",
    "        # This line removes the original text but keeps appending the generated words one-by-one (based on iteration length).\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        if output_prediction_debug:\n",
    "            print(f\"Contents of 'out' after .tolist(): {out}\\n\")\n",
    "            print(f\"Length of context tokens:{len(context_tokens)}\\n\")\n",
    "\n",
    "        # Outputs the result of the text modeling and prediction.\n",
    "        for o in out:\n",
    "            # Decode - convert from token ID's back into English words.\n",
    "            text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "        #     text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "            print(f\"Content of text: ##text_start_marker##{text}##text_end_marker##\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The usual main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function.\n",
    "    \"\"\"\n",
    "    main_debug = False\n",
    "    num_predictions = 3 # Specify the number of predictions to make for input string.\n",
    "    \n",
    "    # Raw text string.\n",
    "    raw_text = \"this is a test string for trying to understand what the heck is happening in run_generation.py.\"\n",
    "    \n",
    "    # Encode raw text.\n",
    "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "    # Generate and output text prediction results.\n",
    "    output_prediction(num_predictions, context_tokens)\n",
    "    \n",
    "    if main_debug:\n",
    "        print(f\"Raw text: {raw_text}\\n\")\n",
    "        print(f\"Context tokens: {context_tokens}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute to  utilize GPT2 model to generate text prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

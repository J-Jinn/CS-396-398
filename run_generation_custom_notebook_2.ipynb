{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-396 Perpetual Work-in-Progress Status Report 2\n",
    "## Author: Joseph Jinn\n",
    "\n",
    "<br>\n",
    "\n",
    "### Note: I need more coffee..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- https://github.com/dunovank/jupyter-themes\n",
    " - (Jupyter Notebook Themes)\n",
    "\n",
    "- https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29\n",
    " - (useful additions for Jupyter Notebook)\n",
    "\n",
    "- https://medium.com/@rbmsingh/making-jupyter-dark-mode-great-5adaedd814db\n",
    " - (Jupyter dark-mode settings - my eyes are no longer bleeding...)\n",
    "\n",
    "- https://github.com/ipython-contrib/jupyter_contrib_nbextensions\n",
    " - (Jupyter extensions)\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    " - (PyTorch tutorial on character-level RNN)\n",
    " \n",
    "<br>\n",
    "\n",
    "Enter this in Terminal (for use with jupyter-themes):\n",
    "\n",
    "jt -t monokai -f fira -fs 13 -nf ptsans -nfs 11 -N -kl -cursw 5 -cursc r -cellw 95% -T\n",
    "\n",
    "<br>\n",
    "\n",
    "Important files to reference:\n",
    "\n",
    "- modeling_gpt2.py\n",
    " - The GPT2 model source code.\n",
    " \n",
    "- tokenization_gpy2.py\n",
    " - The tokenizer class for the GPT2 model.\n",
    " \n",
    " <br>\n",
    " \n",
    "Reference Material to understand the Theoretical Foundation of GPT2:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Language_model\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "It would also be helpful to have some concept about beam search… I’m not super-happy with what my Googling obtains but…\n",
    "\n",
    "https://en.wikipedia.org/wiki/Beam_search\n",
    "\n",
    "https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    " \n",
    " <br>\n",
    " \n",
    "Also maybe helpful but don’t get distracted:\n",
    "\n",
    "the first 20 minutes or so of this (everything after that is details of training, skip it.)  \n",
    "\n",
    "https://www.youtube.com/watch?v=Keqep_PKrY8\n",
    "\n",
    "https://medium.com/syncedreview/language-model-a-survey-of-the-state-of-the-art-technology-64d1a2e5a466\n",
    "\n",
    "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "\n",
    "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Notes:\n",
    "\n",
    "- CTRL + M + L (while in command mode): Adds code cell line numbers (very useful for debugging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Current Progress:\n",
    "\n",
    "Placeholder text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import trange # Instantly make your loops show a smart progress meter\n",
    "\n",
    "import torch # Pytorch.\n",
    "import torch.nn.functional as F\n",
    "import numpy as np # Numpy.\n",
    "\n",
    "###############################################\n",
    "\n",
    "# Hugging-face Transformers.\n",
    "from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from transformers import CTRLLMHeadModel, CTRLTokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the GPT2-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = GPT2LMHeadModel # Specifies the model to use.\n",
    "tokenizer_class = GPT2Tokenizer # Specifies the tokenizer to use for the model.\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2') # Use pre-trained model.\n",
    "model = model_class.from_pretrained('gpt2') # User pre-trained model.\n",
    "model.to('cpu') # Specifies what machine to run the model on.\n",
    "model.eval() # Specifies that the model is NOT in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "# DIVIDER\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k_tokens(filtered_logits, k_value):\n",
    "    \"\"\"\n",
    "    This function utilizes the torch.topk() function to choose the \"k\" most likely words.\n",
    "    \n",
    "    torch.topk performs a similar function to Softmax and argmax.\n",
    "    Use the words' \"scores\" to choose the top \"k\" most likely predicted words (tokens).\n",
    "\n",
    "    - torch.topk\n",
    "     - Returns the :attr:`k` largest elements of the given :attr:`input` tensor along a given dimension.\n",
    "\n",
    "    Non-statistical and probabilistic method, so results are deterministic (always the same).\n",
    "    \n",
    "    Parameters:\n",
    "        filtered_logits - entire vocabulary with assigned scores from GPT2 model.\n",
    "        k_value - choose \"k\" top most likely words.\n",
    "    \n",
    "    Return:\n",
    "        my_topk - top \"k\" word tokens.\n",
    "    \"\"\"\n",
    "    topk_debug = False\n",
    "\n",
    "    # Return the top \"k\" most likely (highest score value) words in sorted order..\n",
    "    my_topk = torch.topk(input=filtered_logits, k=k_value, dim=1, sorted=True)\n",
    "    if topk_debug:\n",
    "        print(f\"My torch.topk object: {my_topk}\\n\")\n",
    "        print(f\"torch.topk indices: {my_topk.indices}\")\n",
    "        print(f\"torch.topk values: {my_topk.values}\\n\")\n",
    "\n",
    "    # https://stackoverflow.com/questions/34750268/extracting-the-top-k-value-indices-from-a-1-d-tensor\n",
    "    # https://stackoverflow.com/questions/53903373/convert-pytorch-tensor-to-python-list\n",
    "\n",
    "    # Indices = encoded words, Values = scores.\n",
    "    if topk_debug:\n",
    "        print(f\"\\nDecoded torch.topk indices: {[tokenizer.decode(idx) for idx in my_topk.indices.squeeze().tolist()]}\")\n",
    "        print(f\"\\nDecoded torch.topk values: {tokenizer.decode(my_topk.indices.squeeze().tolist())}\\n\")\n",
    "\n",
    "        print(f\"topk indices shape: {my_topk.indices.shape}\")\n",
    "        print(f\"topk indices shape after squeeze: {my_topk.indices.squeeze().shape}\")\n",
    "        print(f\"topk indices after squeeze: {my_topk.indices.squeeze()}\\n\")\n",
    "\n",
    "        # TODO: Ask Professor Arnold how to add/remove dimensions to a PyTorch Tensor.\n",
    "        # https://stackoverflow.com/questions/43328632/pytorch-reshape-tensor-dimension\n",
    "        print(f\"topk indices 1st element in Tensor: {my_topk.indices[0][0]}\")\n",
    "        print(f\"topk indices 1st element in Tensor shape: {my_topk.indices[0][0].shape}\")\n",
    "        print(f\"topk indices 1st element in Tensor with added dimension: {my_topk.indices[0][0].unsqueeze(0)}\")\n",
    "        print(f\"topk indices 1st element in Tensor with added dimension shape: {my_topk.indices[0][0].unsqueeze(0).shape}\\n\")\n",
    "\n",
    "    if topk_debug:\n",
    "        # Ghetto looping through topk indices.\n",
    "        for elements in my_topk.indices[0]:\n",
    "            if topk_debug:\n",
    "                print(f\"topk word: {elements}\")\n",
    "                print(f\"topk word shape: {elements.shape}\")\n",
    "                print(f\"topk word shape after unsqueezing: {elements.unsqueeze(0).unsqueeze(0).shape}\")\n",
    "\n",
    "            # Set each element as the next token for text prediction and generation.\n",
    "            next_token = elements.unsqueeze(0).unsqueeze(0)\n",
    "            if topk_debug:\n",
    "                print(f\"Next token shape: {next_token.shape}\")\n",
    "                print(f\"Next token: {next_token}\")\n",
    "                print(f\"Decoded next token(s): {tokenizer.decode(next_token.squeeze().tolist())}\\n\")\n",
    "            \n",
    "    # Returns the Tensor array of the top \"k\" word tokens\n",
    "    return my_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main encodes the raw text string, wraps in PyTorch Tensor, and calls prediction_generation().\n",
    "    \n",
    "    Parameters: \n",
    "        None\n",
    "    Return: \n",
    "        None\n",
    "    \"\"\"\n",
    "    main_debug = False\n",
    "    context_debug = False\n",
    "    num_samples = 1 # Default value.\n",
    "    \n",
    "    raw_text = \"this is a test string for trying to understand what the heck is happening in run_generation.py.\"\n",
    "    \n",
    "    # Encode raw text.\n",
    "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "    \n",
    "    if main_debug:\n",
    "        print(f\"Raw text: {raw_text}\\n\")\n",
    "        print(f\"Context tokens: {context_tokens}\\n\")\n",
    "    \n",
    "    context = context_tokens # Set to name as in run_generation.py\n",
    "    \n",
    "    # Convert to a PyTorch Tensor object (numpy array).\n",
    "    context = torch.tensor(context, dtype=torch.long, device='cpu')\n",
    "    if context_debug:\n",
    "        print(f\"Context shape: {context.shape}\")\n",
    "        print(f\"Context converted to Pytorch Tensor object: {context}\\n\")\n",
    "\n",
    "    # Unsqueeze adds a dimension to the Tensor array.\n",
    "    # Repeat adds x-dimensions and repeats the Tensor elements y-times.\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    if context_debug:\n",
    "        print(f\"Context shape after 'unsqueeze': {context.shape}\")\n",
    "        print(f\"Context after 'unsqueeze': {context}\\n\")\n",
    "\n",
    "    generated = context # Set to name as in run_generation.py\n",
    "    \n",
    "    # Generate and output text prediction results.\n",
    "    prediction_generation(context_tokens, generated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: ['\\n It The If I']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token: \n",
      "\n",
      "Choose a Token:  It\n",
      "Choose a Token:  The\n",
      "Choose a Token:  If\n",
      "Choose a Token:  I\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text:\n",
      ":\n",
      "\n",
      "Predicted text: It:\n",
      "\n",
      "Predicted text: The:\n",
      "\n",
      "Predicted text: If:\n",
      "\n",
      "Predicted text: I:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' test first output tests code']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  test\n",
      "Choose a Token:  first\n",
      "Choose a Token:  output\n",
      "Choose a Token:  tests\n",
      "Choose a Token:  code\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The test:\n",
      "\n",
      "Predicted text: The first:\n",
      "\n",
      "Predicted text: The output:\n",
      "\n",
      "Predicted text: The tests:\n",
      "\n",
      "Predicted text: The code:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' are will should for can']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  are\n",
      "Choose a Token:  will\n",
      "Choose a Token:  should\n",
      "Choose a Token:  for\n",
      "Choose a Token:  can\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests are:\n",
      "\n",
      "Predicted text: The tests will:\n",
      "\n",
      "Predicted text: The tests should:\n",
      "\n",
      "Predicted text: The tests for:\n",
      "\n",
      "Predicted text: The tests can:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' be run return work not']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  be\n",
      "Choose a Token:  run\n",
      "Choose a Token:  return\n",
      "Choose a Token:  work\n",
      "Choose a Token:  not\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should be:\n",
      "\n",
      "Predicted text: The tests should run:\n",
      "\n",
      "Predicted text: The tests should return:\n",
      "\n",
      "Predicted text: The tests should work:\n",
      "\n",
      "Predicted text: The tests should not:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' fine, as in on']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  fine\n",
      "Choose a Token: ,\n",
      "Choose a Token:  as\n",
      "Choose a Token:  in\n",
      "Choose a Token:  on\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work fine:\n",
      "\n",
      "Predicted text: The tests should work,:\n",
      "\n",
      "Predicted text: The tests should work as:\n",
      "\n",
      "Predicted text: The tests should work in:\n",
      "\n",
      "Predicted text: The tests should work on:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' the all any a Python']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  the\n",
      "Choose a Token:  all\n",
      "Choose a Token:  any\n",
      "Choose a Token:  a\n",
      "Choose a Token:  Python\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in the:\n",
      "\n",
      "Predicted text: The tests should work in all:\n",
      "\n",
      "Predicted text: The tests should work in any:\n",
      "\n",
      "Predicted text: The tests should work in a:\n",
      "\n",
      "Predicted text: The tests should work in Python:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 6/20 [00:00<00:02,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' similar single test way separate']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  similar\n",
      "Choose a Token:  single\n",
      "Choose a Token:  test\n",
      "Choose a Token:  way\n",
      "Choose a Token:  separate\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a similar:\n",
      "\n",
      "Predicted text: The tests should work in a single:\n",
      "\n",
      "Predicted text: The tests should work in a test:\n",
      "\n",
      "Predicted text: The tests should work in a way:\n",
      "\n",
      "Predicted text: The tests should work in a separate:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▌      | 7/20 [00:01<00:01,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' case suite- environment context']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  case\n",
      "Choose a Token:  suite\n",
      "Choose a Token: -\n",
      "Choose a Token:  environment\n",
      "Choose a Token:  context\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test case:\n",
      "\n",
      "Predicted text: The tests should work in a test suite:\n",
      "\n",
      "Predicted text: The tests should work in a test-:\n",
      "\n",
      "Predicted text: The tests should work in a test environment:\n",
      "\n",
      "Predicted text: The tests should work in a test context:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 8/20 [00:01<00:01,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [',. and that but']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token: ,\n",
      "Choose a Token: .\n",
      "Choose a Token:  and\n",
      "Choose a Token:  that\n",
      "Choose a Token:  but\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context,:\n",
      "\n",
      "Predicted text: The tests should work in a test context.:\n",
      "\n",
      "Predicted text: The tests should work in a test context and:\n",
      "\n",
      "Predicted text: The tests should work in a test context that:\n",
      "\n",
      "Predicted text: The tests should work in a test context but:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: ['\\n The If This Run']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token: \n",
      "\n",
      "Choose a Token:  The\n",
      "Choose a Token:  If\n",
      "Choose a Token:  This\n",
      "Choose a Token:  Run\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context.\n",
      ":\n",
      "\n",
      "Predicted text: The tests should work in a test context. The:\n",
      "\n",
      "Predicted text: The tests should work in a test context. If:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This:\n",
      "\n",
      "Predicted text: The tests should work in a test context. Run:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 10/20 [00:01<00:01,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' is means test will should']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  is\n",
      "Choose a Token:  means\n",
      "Choose a Token:  test\n",
      "Choose a Token:  will\n",
      "Choose a Token:  should\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This is:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This means:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This test:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This will:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' be not work also only']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  be\n",
      "Choose a Token:  not\n",
      "Choose a Token:  work\n",
      "Choose a Token:  also\n",
      "Choose a Token:  only\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should be:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should work:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should also:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should only:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 12/20 [00:01<00:01,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' be happen cause work affect']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  be\n",
      "Choose a Token:  happen\n",
      "Choose a Token:  cause\n",
      "Choose a Token:  work\n",
      "Choose a Token:  affect\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not be:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not happen:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not work:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not affect:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▌   | 13/20 [00:01<00:01,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' any problems a issues an']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  any\n",
      "Choose a Token:  problems\n",
      "Choose a Token:  a\n",
      "Choose a Token:  issues\n",
      "Choose a Token:  an\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause any:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause problems:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause a:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause issues:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 14/20 [00:02<00:00,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' error issue exception unexpected infinite']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  error\n",
      "Choose a Token:  issue\n",
      "Choose a Token:  exception\n",
      "Choose a Token:  unexpected\n",
      "Choose a Token:  infinite\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an error:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an issue:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an exception:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an unexpected:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' loop regress number amount error']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  loop\n",
      "Choose a Token:  regress\n",
      "Choose a Token:  number\n",
      "Choose a Token:  amount\n",
      "Choose a Token:  error\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite loop:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite regress:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite number:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite error:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 16/20 [00:02<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' of ( or more out']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  of\n",
      "Choose a Token:  (\n",
      "Choose a Token:  or\n",
      "Choose a Token:  more\n",
      "Choose a Token:  out\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount of:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount or:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount more:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount out:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: ['orifunlesseas']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token: or\n",
      "Choose a Token: if\n",
      "Choose a Token: unless\n",
      "Choose a Token: e\n",
      "Choose a Token: as\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (or:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (if:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (unless:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (e:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [' it the you in we']\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  it\n",
      "Choose a Token:  the\n",
      "Choose a Token:  you\n",
      "Choose a Token:  in\n",
      "Choose a Token:  we\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as it:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as the:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as you:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as in:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as we:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████▌| 19/20 [00:03<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top k=5 tokens are: [\" have will are've can\"]\n",
      "Note: If chosen token does not exist, you will see this prompt repeat forever and ever...\n",
      "Choose a Token:  have\n",
      "Choose a Token:  will\n",
      "Choose a Token:  are\n",
      "Choose a Token: 've\n",
      "Choose a Token:  can\n",
      "Original raw text string: this is a test string for trying to understand what the heck is happening in run_generation.py.\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as we have:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as we will:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as we are:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as we've:\n",
      "\n",
      "Predicted text: The tests should work in a test context. This should not cause an infinite amount (as we can:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def prediction_generation(context_tokens, generated):\n",
    "    \"\"\"\n",
    "    This function makes text prediction using the GPT2 model and outputs the results.\n",
    "    \n",
    "    Parameters:\n",
    "       context_tokens - the encoded raw text string.\n",
    "       generated - context_tokens wrapped as a PyTorch Tensor.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    temperature = 1 # Default value.\n",
    "    iterations =  20 # Default value.\n",
    "    k_value = 5 # Top \"k\" words to choose.\n",
    "    generated_array = [] # List of \"generated\" PyTorch Tensor containing encoded word tokens.\n",
    "    token_score_array = [] # List of \"scores\" for each token in the current iteration of topk.\n",
    "    logits_debug = False\n",
    "    topk_debug = False\n",
    "    output_debug = False\n",
    "    \n",
    "    # Create list of PyTorch Tensors containing encoded original raw text string.\n",
    "    for i in range (0, k_value):\n",
    "        generated_array.append(generated)\n",
    "        token_score_array.append(1.)\n",
    "\n",
    "    with torch.no_grad(): # This specifies not to use stochastic gradient descent!\n",
    "        for _ in trange(iterations): \n",
    "              \n",
    "            # Note: Feeding the results back into the model is the beginnings of a beam search algorithm.\n",
    "            # Currently, randomly chooses one of the \"generated\" Tensors to feed back in.\n",
    "            if logits_debug:\n",
    "                print(f\"Original generated shape: {generated}\")\n",
    "                print(f\"Generated array element 0 shape: {generated_array[0]}\")\n",
    "                print(f\"token_score_array element 0 shape: {token_score_array[0]}\\n\")\n",
    "            \n",
    "            # Call to GPT2 model generates a Tensor object containing \"scores\" for the entire vocabulary.\n",
    "            chosen_generated = generated_array[random.randint(0, k_value - 1)]\n",
    "            outputs = model(input_ids=chosen_generated)\n",
    "            if logits_debug:\n",
    "                print(f\"Outputs shape: {list(outputs)[0].shape}\\n\")\n",
    "                print(f\"Outputs: {list(outputs)[0]}\\n\") # Outputs is a tensor containing a lot of stuff...\n",
    "\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "            if logits_debug:\n",
    "                print(f\"Next token logits shape: {next_token_logits.shape}\\n\")\n",
    "                print(f\"Next token logits: {next_token_logits}\\n\")\n",
    "\n",
    "            filtered_logits = next_token_logits # Set to default name from run_generation.py\n",
    "\n",
    "            ############################################################################################\n",
    "\n",
    "            # Call function to extract the top \"k\" word tokens based on their scores.\n",
    "            my_topk = extract_top_k_tokens(filtered_logits, k_value)\n",
    "\n",
    "            counter = 0\n",
    "            # Ghetto looping through topk indices.\n",
    "            for elements in my_topk.indices[0]:\n",
    "                if topk_debug:\n",
    "                    print(f\"topk word: {elements}\")\n",
    "                    print(f\"topk word shape: {elements.shape}\")\n",
    "                    print(f\"topk word shape after unsqueezing: {elements.unsqueeze(0).unsqueeze(0).shape}\")\n",
    "\n",
    "                # Set each element as the next token for text prediction and generation.\n",
    "                next_token = elements.unsqueeze(0).unsqueeze(0)\n",
    "                if topk_debug:\n",
    "                    print(f\"Next token shape: {next_token.shape}\")\n",
    "                    print(f\"Next token: {next_token}\")\n",
    "                    print(f\"Decoded next token(s): {tokenizer.decode(next_token.squeeze().tolist())}\\n\")\n",
    "                \n",
    "                # Concatenate the chosen token (predicted word) to the end of the tokenized (encoded) string.\n",
    "                # Then, add to the array of \"generated\" PyTorch tensors by modifying the original generated structures.\n",
    "                generated_array[counter] = (torch.cat((chosen_generated, next_token), dim=1))\n",
    "                if topk_debug:\n",
    "                    print(f\"Generated shape: {chosen_generated.shape}\")\n",
    "                    print(f\"Generated: {chosen_generated}\")\n",
    "                    print(f\"Decoded 'generated' tokens: {tokenizer.decode(chosen_generated.squeeze().tolist())}\\n\")\n",
    "                    \n",
    "                counter += 1\n",
    "            \n",
    "            # Store the scores for each token.\n",
    "            counter = 0\n",
    "            for elements in my_topk.values[0]:\n",
    "                token_score_array[counter] = elements.unsqueeze(0).unsqueeze(0)\n",
    "                if topk_debug:\n",
    "                    print(f\"topk word score: {elements}\")\n",
    "                    print(f\"topk word score shape: {elements.shape}\")\n",
    "                    print(f\"topk word score shape after unsqueezing: {elements.unsqueeze(0).unsqueeze(0).shape}\")\n",
    "                counter += 1\n",
    "\n",
    "            # Output the text prediction results.\n",
    "            print(f\"Original raw text string: {tokenizer.decode(context_tokens)}\\n\")\n",
    "            for gen in generated_array:\n",
    "                out = gen\n",
    "                if output_debug:\n",
    "                    print(f\"Contents of 'out': {out}\")\n",
    "\n",
    "                # This line removes the original text but keeps appending the generated words one-by-one (based on iteration length).\n",
    "                out = out[:, len(context_tokens):].tolist()\n",
    "                if output_debug:\n",
    "                    print(f\"Contents of 'out' after .tolist(): {out}\\n\")\n",
    "                    print(f\"Length of context tokens:{len(context_tokens)}\\n\")\n",
    "\n",
    "                # Outputs the result of the text modeling and prediction.\n",
    "                for o in out:\n",
    "                    # Decode - convert from token ID's back into English words.\n",
    "                    text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "                #     text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "                    print(f\"Predicted text:{text}:\\n\")\n",
    "                \n",
    "# Execute the program.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "# ABOVE = MY PLAYGROUND\n",
    "# BELOW = FINAL IMPLEMENTATION OF PLAYGROUND FEATURES\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generates the text (word) predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(context):\n",
    "    \"\"\"\n",
    "    This function generates the predicted text and returns the text as tokens.\n",
    "\n",
    "    Parameters: None\n",
    "    Return: generated - PyTorch Tensor containing tokens.\n",
    "    \"\"\"\n",
    "    make_prediction_debug = False\n",
    "    temperature = 1 # Default value.\n",
    "    length = 20 # Default value.\n",
    "    num_samples = 1 # Default value.\n",
    "    \n",
    "    # Convert to a PyTorch Tensor object (numpy array).\n",
    "    context = torch.tensor(context, dtype=torch.long, device='cpu')\n",
    "    if make_prediction_debug:\n",
    "        print(f\"Context shape: {context.shape}\")\n",
    "        print(f\"Context converted to Pytorch Tensor object: {context}\\n\")\n",
    "        \n",
    "    # Unsqueeze adds a dimension to the Tensor array.\n",
    "    # Repeat adds x-dimensions and repeats the Tensor elements y-times.\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    if make_prediction_debug:\n",
    "        print(f\"Context shape after 'unsqueeze': {context.shape}\")\n",
    "        print(f\"Context after 'unsqueeze': {context}\\n\")\n",
    "    \n",
    "    generated = context # Set to name as in run_generation.py\n",
    "    \n",
    "    ############################################################################################\n",
    "\n",
    "    with torch.no_grad(): # This specifies not to use stochastic gradient descent!\n",
    "        for _ in trange(length): \n",
    "            \n",
    "            # Call to GPT2 model generates a Tensor object containing \"scores\" for the entire vocabulary.\n",
    "            outputs = model(input_ids=generated)\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Outputs shape: {list(outputs)[0].shape}\\n\")\n",
    "                print(f\"Outputs: {list(outputs)[0]}\\n\") # Outputs is a tensor containing a lot of stuff...\n",
    "\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Next token logits shape: {next_token_logits.shape}\\n\")\n",
    "                print(f\"Next token logits: {next_token_logits}\\n\")\n",
    "\n",
    "            filtered_logits = next_token_logits # Set to default name from run_generation.py\n",
    "\n",
    "            ############################################################################################\n",
    "            \n",
    "            \"\"\"\n",
    "            torch.topk performs a similar function to Softmax and argmax.\n",
    "                Use the words' \"scores\" to choose the top \"k\" most likely predicted words (tokens).\n",
    "\n",
    "            - torch.topk\n",
    "             - Returns the :attr:`k` largest elements of the given :attr:`input` tensor along a given dimension.\n",
    "\n",
    "            Non-statistical and probabilistic method, so results are deterministic (always the same).\n",
    "            \"\"\"\n",
    "            # Return the top \"k\" most likely (highest score value) words in sorted order..\n",
    "            my_topk = torch.topk(input=filtered_logits, k=1, dim=1, sorted=True)\n",
    "            print(f\"My torch.topk object: {my_topk}\\n\")\n",
    "            print(f\"torch.topk indices: {my_topk.indices}\\n\")\n",
    "            print(f\"torch.topk values: {my_topk.values}\\n\")\n",
    "\n",
    "            # https://stackoverflow.com/questions/34750268/extracting-the-top-k-value-indices-from-a-1-d-tensor\n",
    "            # https://stackoverflow.com/questions/53903373/convert-pytorch-tensor-to-python-list\n",
    "\n",
    "            # \n",
    "            print(f\"\\nDecoded torch.topk indices: {[tokenizer.decode(idx) for idx in my_topk.indices.squeeze().tolist()]}\\n\")\n",
    "            print(f\"\\nDecoded torch.topk values: {tokenizer.decode(my_topk.indices.squeeze().tolist())}\\n\")\n",
    "            \n",
    "            ############################################################################################\n",
    "            \n",
    "            # Concatenate the chosen token (predicted word) to the end of the tokenized (encoded) string.\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            if make_prediction_debug:\n",
    "                print(f\"Generated shape: {generated.shape}\")\n",
    "                print(f\"Generated: {generated}\")\n",
    "                print(f\"Decoded 'generated' tokens: {tokenizer.decode(generated.squeeze().tolist())}\\n\")\n",
    "                                       \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calls the make_prediction(context) function and outputs text prediction and generation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def output_prediction(num_predictions, context_tokens):\n",
    "    \"\"\"\n",
    "    This function outputs the results of our generated predicted text.\n",
    "    \"\"\"\n",
    "    output_prediction_debug = False\n",
    "    \n",
    "    for i in range(0, num_predictions):\n",
    "            \n",
    "        out = make_prediction(context_tokens) # Function returns \"generated\" - PyTorch Tensor containing encoded tokens.\n",
    "        if output_prediction_debug:\n",
    "            print(f\"Contents of 'out': {out}\")\n",
    "\n",
    "        # This line removes the original text but keeps appending the generated words one-by-one (based on iteration length).\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        if output_prediction_debug:\n",
    "            print(f\"Contents of 'out' after .tolist(): {out}\\n\")\n",
    "            print(f\"Length of context tokens:{len(context_tokens)}\\n\")\n",
    "\n",
    "        # Outputs the result of the text modeling and prediction.\n",
    "        for o in out:\n",
    "            # Decode - convert from token ID's back into English words.\n",
    "            text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "        #     text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "            print(f\"Content of text: ##text_start_marker##{text}##text_end_marker##\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The usual main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function.\n",
    "    \"\"\"\n",
    "    main_debug = False\n",
    "    num_predictions = 3 # Specify the number of predictions to make for input string.\n",
    "    \n",
    "    raw_text = \"this is a test string for trying to understand what the heck is happening in run_generation.py.\"\n",
    "    \n",
    "    # Encode raw text.\n",
    "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "    # Generate and output text prediction results.\n",
    "    output_prediction(num_predictions, context_tokens)\n",
    "    \n",
    "    if main_debug:\n",
    "        print(f\"Raw text: {raw_text}\\n\")\n",
    "        print(f\"Context tokens: {context_tokens}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute to  utilize GPT2 model to generate text prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
